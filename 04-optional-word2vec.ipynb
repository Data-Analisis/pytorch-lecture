{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "\"Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that a simple mathematical function (the cosine similarity between the vectors) indicates the level of semantic similarity between the words represented by those vectors.\" [https://en.wikipedia.org/wiki/Word2vec]\n",
    "\n",
    "Here we will build a PyTorch model that implements Word2Vec's CBOW strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Daniel_Braun6/publication/326588219/figure/fig1/AS:652185784295425@1532504616288/Continuous-Bag-of-words-CBOW-CB-and-Skip-gram-SG-training-model-illustrations.png\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.lin_out = nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (bs, context_size) -> (bs, context_size, emb_dim)\n",
    "        x = self.emb(x)\n",
    "        \n",
    "        # (bs, context_size, emb_dim) -> (bs, emb_dim)\n",
    "        x = x.sum(dim=1)\n",
    "\n",
    "        # (bs, emb_dim) -> (bs, vocab_size)\n",
    "        logits = self.lin_out(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Instantiate the model and write a proper training loop. Here are some functions to help you make the data ready for use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'are', 'about', 'to', 'study', 'the', 'idea', 'of', 'a', 'computational', 'process.']\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "print(raw_text[:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "{'<pad>': 0, '<unk>': 1, 'they': 2, 'study': 3, 'about': 4, 'the': 5, 'beings': 6, 'by': 7, 'effect,': 8, 'to': 9, 'our': 10, 'spells.': 11, 'The': 12, 'process.': 13, 'processes.': 14, 'are': 15, 'evolve,': 16, 'rules': 17, 'a': 18, 'things': 19, 'People': 20, 'is': 21, 'computers.': 22, 'Computational': 23, 'we': 24, 'computer': 25, 'evolution': 26, 'manipulate': 27, 'As': 28, 'program.': 29, 'of': 30, 'processes': 31, 'inhabit': 32, 'We': 33, 'abstract': 34, 'direct': 35, 'with': 36, 'programs': 37, 'conjure': 38, 'In': 39, 'directed': 40, 'other': 41, 'process': 42, 'data.': 43, 'create': 44, 'pattern': 45, 'idea': 46, 'computational': 47, 'that': 48, 'called': 49, 'spirits': 50}\n",
      "they\n"
     ]
    }
   ],
   "source": [
    "vocab = set(raw_text)\n",
    "\n",
    "\n",
    "# shifted by 2 due to special tokens for padding and unknown tokens\n",
    "word_to_ix = {}\n",
    "word_to_ix['<pad>'] = 0\n",
    "word_to_ix['<unk>'] = 1\n",
    "for i, word in enumerate(vocab):\n",
    "    word_to_ix[word] = i + 2\n",
    "ix_to_word = list(word_to_ix.keys())\n",
    "vocab_size = len(word_to_ix)\n",
    "\n",
    "print(vocab_size)\n",
    "print(word_to_ix)\n",
    "print(ix_to_word[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['We', 'are', 'to', 'study'], 'about')\n",
      "(['are', 'about', 'study', 'the'], 'to')\n",
      "(['about', 'to', 'the', 'idea'], 'study')\n"
     ]
    }
   ],
   "source": [
    "context_size = 2  # 2 words to the left and 2 to the right\n",
    "data = []\n",
    "for i in range(context_size, len(raw_text) - context_size):\n",
    "    context = [raw_text[i + j] for j in range(- context_size, context_size + 1) if j != 0]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "\n",
    "print(data[0])\n",
    "print(data[1])\n",
    "print(data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_ids(context, word_to_ix):\n",
    "    list_of_ids = []\n",
    "    for w in context:\n",
    "        if w in word_to_ix:\n",
    "            list_of_ids.append(word_to_ix[w])\n",
    "        else:\n",
    "            list_of_ids.append(1)  # unknown id = 1\n",
    "    return list_of_ids\n",
    "\n",
    "\n",
    "def get_target_id(target, word_to_ix):\n",
    "    target_word_id = 0\n",
    "    if target in word_to_ix:\n",
    "        target_word_id = word_to_ix[target]\n",
    "    return target_word_id\n",
    "\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = get_list_of_ids(context, word_to_ix)\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 15, 9, 3]\n",
      "4\n",
      "tensor([33, 15,  9,  3])\n"
     ]
    }
   ],
   "source": [
    "print(get_list_of_ids(data[0][0], word_to_ix))\n",
    "print(get_target_id(data[0][1], word_to_ix))\n",
    "print(make_context_vector(data[0][0], word_to_ix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "If you like, these PyTorch's NLP tutorials are a good place to start building NLP models:\n",
    "\n",
    "- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "- https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "- https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
    "- https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "- https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
